# -*- coding: utf-8 -*-
"""tensors_&_SGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H7x_zDjJNhw-ftUeK-kdGlPpsqed00Bw

# Tensors & Stochastic Gradient Descent (SGD) in PyTorch

## Tensors

SGD is one of the most commonly used **optimization** algorithms for deep learning. Here is a simple demonstration of SGD for Linear Regression, one of the simplest, most commonly used and highly interpretable machine learning models.

A simple linear regression can be defined as follows,

$y=aX + b$

$a = [a_1, a_2]$

$ X = [1, 2, 3] $

### Import Libraries
"""

# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch import nn
torch.manual_seed(42); # set for reproducibility

"""### Generating Tensors

https://discuss.pytorch.org/t/what-is-the-difference-between-tensor-and-tensor-is-tensor-going-to-be-deprecated-in-the-future/17134/8
"""

print(torch.tensor(1), '->', torch.tensor(1).dtype)
print(torch.empty(1), '->', torch.empty(1).dtype)
print(torch.FloatTensor(1), '->', torch.FloatTensor(1).dtype)
print(torch.Tensor(1), '->', torch.Tensor(1).dtype)

"""> Note : Here `torch.tensor()` with a single number actually returns a scalar values whereas the other return a tensor.

### Specifying types and coercing
"""

x = torch.tensor(1, dtype=torch.int32)
print(x.dtype)
y = torch.tensor(1., dtype=torch.int32)
print(y.dtype)

torch.tensor([1,1]).size()

"""Set the number of training samples/examples and create a tensor of **n x 2** dimensions as follows,

Cannot do this with `torch.Tensor()`
"""

torch.Tensor(1, dtype=torch.float32)

"""For Tensors with floats,"""

torch.FloatTensor([1, 2])

"""Equivalently,"""

torch.tensor([1.,2]) # The dot at the end signifies all numbers are to be taken as floats.

"""Lets verify with their data types,"""

print(torch.Tensor([1.,2]).dtype) # i.e the default datatype
torch.Tensor([1.,2]).dtype == torch.FloatTensor([1, 2]).dtype

"""Now lets generate a tensor with numbers drawn from a **uniform distribution**,"""

torch.rand([3,5]) # return a tensor of dimension x,y with numbers drawn from a uniform distribution U ~ [0,1)

"""To draw random variables from a uniform distribution,  
$U ~ [r1, r2]$

Replace the first column with numbers drawn from a uniform distribution 
$U ~ [0, 1)$.  
`uniform_` here does operations in inplace (anything with a 
underscore at the end means inplace)

If U is a random variable uniformly distributed on [0, 1], then (r1 - r2) * U + r2 is uniformly distributed on [r1, r2]. Alternatively, (r2 - r1) * U + r1  (https://stackoverflow.com/questions/44328530/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch)

(r1 - r2) * torch.rand(a, b) + r2 or  torch.FloatTensor(a, b).uniform_(r1, r2)
"""

a = 1; b = 2; r1 = -1; r2 = 1
print((r1 - r2) * torch.rand(a, b) + r2)
print(torch.FloatTensor([a, b]).uniform_(r1, r2))

n = 10 # training samples
x = torch.ones(n,2) # Generate vector/matrix of ones
print(x.dtype) # datatype
print(x.size()) # size of tensor. Note : torch.shape is similar to torch.size()

x[:,0].uniform_(-1.,1)
print(x)

"""### Initializing parameters

Note : Writing a dot at the end of a integer coerces it to be a float as follows,
"""

# Initializing the slope and intercept AKA the parameters, weights, coefficients
# of the equation
a = torch.tensor([3.,2]) 
print(a)

"""As Jeremy (fast.ai), pointed out :
A very intuitive, visual resource for matrix multiplications,
> http://matrixmultiplication.xyz/

* `@` in python is an opeartor for matrix multiplication (`@=` is the inplace version)
* `*` is elementwise product between two matrices or vectors
"""

print(x.size()); print(a.size())
print((x@a).size())
x@a

"""Lets sample n random variables from U ~ [0,1)"""

b = torch.rand(n)
print(b)

"""Lets generate y from the equation of the line (linear regression),"""

y = x@a + b
print(y)

sns.scatterplot(x=x[:,0], y=y);

"""A common loss function **Mean Squared Error (MSE)** gives us a measure of how well our model is doing and is given as follows,

$\frac{sum{(y^hat-y)^2}}{n}$
"""

def mse(y_hat, y): 
    
    return torch.mean((y_hat-y)**2)

a = torch.rand(2) # initialize the parameters
print(a)

y_hat = x@a
print(y_hat)

mse(y_hat, y)

sns.scatterplot(x[:,0],y);
sns.scatterplot(x[:,0],y_hat);

a = nn.Parameter(a);
print(a)

"""requires_grad=True would calculate via automatic differentiation"""

def update(lr=1e-03, i=0):
    
    y_hat = x@a
    
    loss = mse(y_hat, y)
    
    if i % 10 == 0: 
        print(loss)
        
    # calculates the gradients/derivatives of the parameters; stores it in .grad
    loss.backward()
    
    with torch.no_grad(): # turn gradient calculation off
        
        a.sub_(lr * a.grad) # inplace subtraction of gradient 
        a.grad.zero_() # clear all gradients

# specify hyperparameters
lr = 1e-3
epochs = 100

# Training loop
for i in range(epochs): 
    update(lr, i)

a

sns.scatterplot(x[:,0],y);
sns.scatterplot(x[:,0], y_hat);
with torch.torch.no_grad():
    print(a)
    sns.scatterplot(x[:,0], x@a);

"""# References
[1] fast.ai v3 Lesson 2
"""